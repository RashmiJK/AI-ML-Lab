## Beginner's Guide to Machine Learning Modeling: A Toolkit

Starting with machine learning can seem overwhelming, but this beginner-friendly framework—developed from insights gained in the second module of my Post Graduate Program in AI & ML journey—will make it easier to tackle problems and build your first models.

| Section                       | Key Concepts                                   | Details                                                              | Remarks |
|-------------------------------|-------------------------------------------------------|----------------------------------------------------------------------|------------------- |
| **Data Handling & Exploration**                              | **Data Overview**                                          |  Import NumPy and Pandas <br> .read_csv, .head(), .tail(), .sample(), .shape, .copy(), .info(), .describe().T, .isnull.sum(), <br> .duplicated.sum(), .nunique(), .unique(), .value_counts(), .groupby(), .nunique() ,.drop()                             |                                                 | |
| | | | |
| **Data Preprocessing**        | **Missing Value Treatment**                           | Techniques for handling missing data <br> .replace()                                | |
|                               | **Outlier Detection/Treatment**                                 | Strategies for dealing with outliers <br> .quantile(0.25), .quantile(0.75)                                 | |
| | | | |
| **Visualization**             | **Univariate**                                        | Import seaborn <br> **Numerical:** Histogram, Boxplot <br> **Categorical:** Countplot    | sns.histplot() <br> sns.boxplot(), df.boxplot(by='column') <br>|
|                               | **Bivariate**                                         | **N-N:** Scatter/Pair (grid of scatter plots), Line, Heat, Joint, Violin <br> **N-C:** Line, Catplot, Boxplot <br> **C-C:** Count, Box, Crosstab + stacked bar chart | sns.heatmap() <br> sns.scatterplot(), px.scatter_3d()  <br> sns.pairplot() <br> plt.plot() for line plot|
|                               | **Multivariate**                                      | Catplot                                                              | |
| | | | |
| **Modeling**                  | **Linear Regression** <br> - Supervised Learning <br> - Predicts numerical target  | Best fit line, method of least squares <br> $Error = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$ <br> $Error = \frac{1}{n} \sum_{i=1}^{n} (y_i - (b_0 + b_1 x_i))^2$ <br> “Coefficients” and “intercept” => model parameters <br> <br> Multiple linear regression => fits hyperplane in higher dimension <br> <br> Metrics <br> - $\mathrm{MAE} = \frac{1}{n} \sum_{i=1}^{n} \left\vert y_i - \hat{y}_i \right\vert$ <br> - $\mathrm{RMSE} = \sqrt{ \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 }$ <br> - $\mathrm{MAPE} = \frac{1}{n} \sum_{i=1}^{n} \left\vert \frac{y_i - \hat{y}_i}{y_i} \right\vert * 100$ <br> - $R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}$  <br> - $\bar{R}^2 = 1 - \left(1 - R^2\right) \frac{n - 1}{n - k - 1}$ <br> - Underfitting, Overfitting                      | - Prepared data <br> - label encoding, one-hot encoding <br> - train_test_split() <br> - LinearRegression() <br> - build models for single feature, combination, all features <br> - r2_score() <br> - mean_absolute_error() <br> - find the best model based on metrics |
|                               | **Decision Tree** <br> - Supervised Learning <br> - Classification <br> - Predicts categorical & numerical targets <br> - Gini impurity (0, pure -> 0.5, high impurity) <br> - Entropy       | Metrics <br> &nbsp; - confusion matrix <br> &nbsp; - $\text{Accuracy} = \frac{\text{TP + TN}}{\text{TP + TN + FP + FN}}$ (Use metrics other than accuracy for imbalanced data) <br> &nbsp; - $\text{Recall} = \frac{\text{TP}}{\text{TP + FN}}$ <br> &nbsp; - $\text{Precision} = \frac{\text{TP}}{\text{TP + FP}}$ <br> &nbsp; - $\text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$ <br> <br> Default Fit <br> <br> Pre-pruning <br> &nbsp; - Hyperparameter tuning <br> &nbsp; - GridSearchCV <br> <br> Post-pruning <br> &nbsp; - Cost complexity (ccp_alpha) <br> $\text{⍺} = \frac{\text{Error(Pruned Tree) - Error(Original Tree)}}{\text{Number of nodes reduced}}$ | - Prepared Data <br> - pd.getdummies() for nominal categorical values, Encoding for ordinal categorical values <br> - train_test_split()  <br> - DecisionTreeClassifier() (HyperP: class_weight, max_depth, max_leaf_nodes, min_samples_split etc) <br> - confusion_matrix() <br> - f1_score() <br> - accuracy_score() <br> - recall_score() <br> - cost_complexity_pruning_path() <br> - precision_score() <br> Find the best fit based on your suitable metric  |
|                               | **Clustering**  <br> - Unsupervised learning <br> - Find patterns                     | Distance metrics <br> - Euclidean <br> - Manhattan  <br> <br> K-Means clustering (centroid) <br> Choose the right K <br> - Elbow method ($WCSS = \sum_{j = 1}^{k} \sum_{x_i \in C_j} (x_i - \overline{x_j})^2$) <br> - Silhouette score ($s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}$)  <br> t-SNE dimensionality reduction and perplexity                                    | Extended EDA + Visualization <br> - Only columns with numeric data types were selected for clustering. <br> - Scaling numerical features (a crucial step in clustering) <br> - StandardScaler() for z-score scaling <br> <br> Use TSNE() <br> - For dimensionality reduction, choose a dimension (2 or 3) <br> - Try with modified perplexity and visualize using scatterplot <br> - This visualization would suggest optimal value for K <br><br> Clustering <br> - KMeans() <br> - .inertia_ -> WCSS metric-> Create an elbow method plot <br> -  Compute Silhouette score (metric) and plot the graph <br> - Reassess the optimal value of K for clustering <br><br> Cluster Profiling by assigning cluster to rows| 
