# A Beginner’s Framework for Building Models

Starting with machine learning can seem overwhelming, but this beginner-friendly framework—developed from insights gained in the second module of my Post Graduate Program in AI & ML journey—will make it easier to tackle problems and build your first models.

| Section | Key Concepts | Details | Implementation |
|---------|--------------|---------|----------------|
| **Data Handling & Exploration** | **Data Overview** | Import NumPy and Pandas<br>`.read_csv()`, `.head()`, `.tail()`, `.sample()`, `.shape`, `.copy()`, `.info()`, `.describe().T`, `.isnull().sum()`, `.duplicated().sum()`, `.nunique()`, `.unique()`, `.value_counts()`, `.groupby()`, `.drop()`, `.loc[df['col'] > value]`, `.iloc[0:5]`, `df['col'].isin(['val1', 'val2'])` | Basic data exploration and manipulation |
| **Data Preprocessing** | **Missing Value Treatment** | Techniques for handling missing data<br>`.select_dtypes([])` <br> `.replace()` method <br>`.fillna()` method <br>`.dropna()` method <br> `.apply()` eg `data['feature'].apply(lambda x : 1 if x=='Yes' else 0)` <br> Additional techniques: <br> 1. converting "objects" to "category" reduces the data space required to store the dataframe| Handle missing values appropriately |
| | **Outlier Detection/Treatment** | Strategies for dealing with outliers<br>- `.quantile(0.25)`, `.quantile(0.75)` <br> - Linear and Logistic regressions are sensitive to outliers. <br> - If were to treat outliers, then values are replaced with median or mean depending on whether data is skewed or not | Quartile analysis for outlier identification |
| **Visualization** | **Univariate Analysis** | Import seaborn<br>**Numerical:** Histogram, Boxplot<br>**Categorical:** Countplot | `sns.histplot()`<br>`sns.boxplot()`, `df.boxplot(by='column')`<br>Single variable visualization <br> `plt.barh()`|
| | **Bivariate Analysis** | **N-N:** Scatter/Pair, Line, Heat, Joint, Violin<br>**N-C:** Line, Catplot, Boxplot<br>**C-C:** Count, Box, Crosstab + stacked bar chart | `sns.heatmap()`<br>`sns.scatterplot()`, `px.scatter_3d()`<br>`sns.pairplot()`<br>`plt.plot()` for line plots |
| | **Multivariate Analysis** | Catplot for complex relationships | Multi-dimensional analysis |
| **Modeling** | **Linear Regression**<br>*Supervised Learning*<br>*Predicts numerical targets* | **Concepts:**<br>• Best fit line, method of least squares<br>• **Error Formula:** $Error = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$<br>• **Expanded:** $Error = \frac{1}{n} \sum_{i=1}^{n} (y_i - (b_0 + b_1 x_i))^2$<br>• Coefficients and intercept = model parameters<br>• Multiple linear regression = hyperplane in higher dimensions<br><br>**Metrics:**<br>• **MAE:** $MAE = \frac{1}{n} \sum_{i=1}^{n} \|y_i - \hat{y}_i\|$<br>• **RMSE:** $RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$<br>• **MAPE:** $MAPE = \frac{1}{n} \sum_{i=1}^{n} \left\|\frac{y_i - \hat{y}_i}{y_i}\right\| \times 100$<br>• **R²:** $R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}$<br>• **Adjusted R²:** $\bar{R}^2 = 1 - \left(1 - R^2\right) \frac{n - 1}{n - k - 1}$<br>• Underfitting, Overfitting | • Prepared data<br>• Label encoding, one-hot encoding<br>• `train_test_split()`<br>• `LinearRegression()`<br>• Build models for single feature, combinations, all features<br>• `r2_score()`<br>• `mean_absolute_error()`<br>• Find best model based on metrics |
|  | **Logistic Regression**<br>*Supervised Learning*<br>*Predicts categorical value* <br> Classification algorithm | **Concepts:**<br>• <br>• **Error Formula:** <br>• **Expanded:** <br>  | • Prepared data<br> |
|  | **Naive Bayes Classifier** | **Concepts:**<br>• <br>• **Error Formula:** <br>• **Expanded:** <br>  | • Prepared data<br> |
|  | **KNN (K-Nearest Neighbors algorithm)** <br> Supervised ML <br> Used for both classification and regression tasks <br>[knn_algorithm_vs_knn_search](./knn_algorithm_vs_knn_search.md)| **Concepts:**<br>• <br>• **Error Formula:** <br>• **Expanded:** <br>  | • Prepared data<br> |
| | **Decision Tree**<br>*Supervised Learning*<br>*Classification*<br>*Predicts categorical & numerical targets* | **Concepts:**<br>• Gini impurity (0 = pure → 0.5 = high impurity)<br>• Entropy<br>• Tree-based decision making<br><br>**Metrics:**<br>• Confusion matrix<br>• **Accuracy:** $Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$ *(Use other metrics for imbalanced data)*<br>• **Recall:** $Recall = \frac{TP}{TP + FN}$<br>• **Precision:** $Precision = \frac{TP}{TP + FP}$<br>• **F1 Score:** $F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}$<br><br>**Optimization:**<br>• Default Fit<br>• **Pre-pruning:** Hyperparameter tuning, GridSearchCV<br>• **Post-pruning:** Cost complexity (ccp_alpha)<br>$\alpha = \frac{Error(Pruned\ Tree) - Error(Original\ Tree)}{Number\ of\ nodes\ reduced}$ | • Prepared data<br>• `pd.get_dummies()` for nominal categorical values<br>• Encoding for ordinal categorical values<br>• `train_test_split()` - stratify parameter for proportionate split for imbalanced data<br>• `DecisionTreeClassifier()` (Hyperparameters: class_weight, max_depth, max_leaf_nodes, min_samples_split, etc.)<br>• `confusion_matrix()`<br>• `f1_score()`, `accuracy_score()`, `recall_score()`, `precision_score()`<br>• `cost_complexity_pruning_path()`<br>• Find best fit based on suitable metric |
| | **Clustering**<br>*Unsupervised Learning*<br>*Pattern Discovery* | **Concepts:**<br>• **Distance metrics:** Euclidean, Manhattan<br>• **K-Means clustering:** Uses centroids<br><br>**Choose optimal K:**<br>• **Elbow method:** $WCSS = \sum_{j=1}^{k} \sum_{x_i \in C_j} (x_i - \overline{x_j})^2$<br>• **Silhouette score:** $s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}$<br><br>**t-SNE:** Dimensionality reduction with perplexity parameter | **Extended EDA + Visualization:**<br>• Select only numeric data types for clustering<br>• **Crucial:** Scale numerical features using `StandardScaler()` for z-score scaling<br><br>**Dimensionality Reduction:**<br>• Use `TSNE()` for 2D or 3D visualization<br>• Try different perplexity values<br>• Visualize using scatterplot<br>• Suggests optimal K value<br><br>**Clustering:**<br>• `KMeans()`<br>• `.inertia_` → WCSS metric → Create elbow method plot<br>• Compute Silhouette score and plot<br>• Reassess optimal K value<br>• Cluster profiling by assigning cluster labels to rows |