## Some Guidelines to choose an LLM for your purpose
Depends on the task and resources available.

### Self-served or API based
### Size and capability tiers

### Metrics and benchmarks
 
1. Chatbot Arena : An Open Platform for Evaluating LLMs by Human Preference (developed by researchers at UC Berkeley SkyLab and LMArena). LMArena is an open-source platform for crowdsourced AI benchmarking, created by researchers from UC Berkeley SkyLab.  
[Chatbot Arena LLM Leaderboard](https://lmarena.ai/?leaderboard)

Numerical comparision of LLMs. Refer to benchmarks. Few as examples below.
2. MT-Bench : A set of challenging multi-turn questions graded by GPT-4.  
3. MMLU : Massive Multitask Language Undertsanding : which accesses world knowledge and problem-solving ability across a range of topics and difficulty levels : a test to measure model's multitask accuracy on 57 tasks, from Abstract Algenra to Virology.  

 Specialized LLMs
 Reasoning
 Multilinguality
 Safety guardrails

 ###Â Benchmarks
 MTEB - Massive text embedding benchmark