{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19092e52",
   "metadata": {},
   "source": [
    "# Cleaning textual data\n",
    "Popular python libraries used in cleaning textual data are **NLTK, re, sklearn, pandas**.\n",
    "\n",
    "Structured : Data is organized into pre-defined structure like a table of database - with rows and columns.\n",
    "Unstructured data : Data does not have a pre-defined structure. Eg: emails, a bunch of satellite images, text if speeches.\n",
    "\n",
    "Converting Unstructured Data into Structured Form - \n",
    "1. Bag of Words: A method for text representation that lists all words in a document, disregarding order, to simplify text analysis.  \n",
    "e.g: Bayesian Spam Filter utilizes the bag of words model to classify emails by analyzing word frequency patterns to filter spam. Further understanding of Naive Bayes : https://www.youtube.com/watch?v=O2L2Uv9pdDA\n",
    "\n",
    "2. N-grams: An extension of the bag of words model, n-grams analyze sequences of words to capture spatial relationships, providing more context.\n",
    "\n",
    "3. Semantic Methods: These methods interpret text by understanding language structure and grammar, allowing for deeper contextual insights.  \n",
    "e.g: Name Entity Identification: A semantic technique for recognizing and categorizing key entities like names of people, places, and organizations within text.\n",
    "\n",
    "Common text preprocessing techniques:\n",
    "1. Stop-word Removal: This involves eliminating common words that don’t add significant meaning to the text, such as “the”, “and”, and “in”, to focus on more meaningful words.\n",
    "2. Stemming: This reduces words to their base or root form, treating different word forms as the same entity, which is useful in text analysis.\n",
    "3. Case Conversion: This involves changing all text to a uniform case, either lower or upper, ensuring consistent treatment of words regardless of their original case.\n",
    "4. Punctuation and White Space Removal: This step removes punctuation and extra white spaces since they do not contribute to the meaning in a bag of words model, preventing inconsistencies.\n",
    "5. Number Removal: This involves removing numbers when they do not add significant meaning to the text, simplifying the analysis.\n",
    "6. Word Frequency and Bag of Words Model: This technique represents text data by counting word frequency in a document, aiding in text classification and clustering by providing a simple way to quantify text data.  \n",
    "These techniques aim to clean and prepare text data for analysis by simplifying the text while retaining its core meaning, enhancing the efficiency of natural language processing tasks.\n",
    "\n",
    "Sentiment Analysis, which involves analyzing texts to understand the sentiment expressed. The main approaches include lexicon-based and machine learning-based techniques. Sentiment Analysis can be applied in areas like product and movie reviews and is useful for monitoring customer feedback and market research.\n",
    "\n",
    "a lexicon refers to a predefined list of words, each associated with specific sentiments. It is used in sentiment analysis, where each word within a text is replaced with its corresponding sentiment from the lexicon. This process helps summarize the overall sentiment expressed in the text. The effectiveness of this approach heavily depends on the quality of the lexicon used, and one of its challenges is dealing with words that have multiple interpretations depending on the context."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
