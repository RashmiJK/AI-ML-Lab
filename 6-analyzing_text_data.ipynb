{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19092e52",
   "metadata": {},
   "source": [
    "# Cleaning textual data\n",
    "Popular python libraries used in cleaning textual data are **NLTK, re, sklearn, pandas**.\n",
    "\n",
    "Structured : Data is organized into pre-defined structure like a table of database - with rows and columns.\n",
    "Unstructured data : Data does not have a pre-defined structure. Eg: emails, a bunch of satellite images, text if speeches.\n",
    "\n",
    "Converting Unstructured Data into Structured Form - \n",
    "1. Bag of Words: A method for text representation that lists all words in a document, disregarding order, to simplify text analysis.  \n",
    "e.g: Bayesian Spam Filter utilizes the bag of words model to classify emails by analyzing word frequency patterns to filter spam. Further understanding of Naive Bayes : https://www.youtube.com/watch?v=O2L2Uv9pdDA\n",
    "\n",
    "2. N-grams: An extension of the bag of words model, n-grams analyze sequences of words to capture spatial relationships, providing more context.\n",
    "\n",
    "3. Semantic Methods: These methods interpret text by understanding language structure and grammar, allowing for deeper contextual insights.  \n",
    "e.g: Name Entity Identification: A semantic technique for recognizing and categorizing key entities like names of people, places, and organizations within text.\n",
    "\n",
    "Common text preprocessing techniques:\n",
    "1. Stop-word Removal: This involves eliminating common words that don’t add significant meaning to the text, such as “the”, “and”, and “in”, to focus on more meaningful words.\n",
    "2. Stemming: This reduces words to their base or root form, treating different word forms as the same entity, which is useful in text analysis.\n",
    "\n",
    "    **Note : Stemming and lemmatization are both text normalization techniques in Natural Language Processing (NLP) that reduce words to their root form, but they differ in their approach. Stemming is a simpler, rule-based process that often truncates word endings, potentially leading to non-dictionary words. Lemmatization, on the other hand, considers the context and morphological analysis of a word to return its dictionary-based root form (lemma), which is always a valid word.** \n",
    "\n",
    "\n",
    "3. Case Conversion: This involves changing all text to a uniform case, either lower or upper, ensuring consistent treatment of words regardless of their original case.\n",
    "4. Punctuation and White Space Removal: This step removes punctuation and extra white spaces since they do not contribute to the meaning in a bag of words model, preventing inconsistencies.\n",
    "5. Number Removal: This involves removing numbers when they do not add significant meaning to the text, simplifying the analysis.\n",
    "6. Word Frequency and Bag of Words Model: This technique represents text data by counting word frequency in a document, aiding in text classification and clustering by providing a simple way to quantify text data.  \n",
    "These techniques aim to clean and prepare text data for analysis by simplifying the text while retaining its core meaning, enhancing the efficiency of natural language processing tasks.\n",
    "\n",
    "Sentiment Analysis, which involves analyzing texts to understand the sentiment expressed. The main approaches include lexicon-based and machine learning-based techniques. Sentiment Analysis can be applied in areas like product and movie reviews and is useful for monitoring customer feedback and market research.\n",
    "\n",
    "a lexicon refers to a predefined list of words, each associated with specific sentiments. It is used in sentiment analysis, where each word within a text is replaced with its corresponding sentiment from the lexicon. This process helps summarize the overall sentiment expressed in the text. The effectiveness of this approach heavily depends on the quality of the lexicon used, and one of its challenges is dealing with words that have multiple interpretations depending on the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe2a73d",
   "metadata": {},
   "source": [
    "| Pattern Syntax | Description |\n",
    "|---------------|-------------|\n",
    "| `[a-z]` | Matches any lowercase letter |\n",
    "| `[A-Z]` | Matches any uppercase letter |\n",
    "| `[0-9]` | Matches any digit |\n",
    "| `[^0-9]` | Matches any character except digits 0-9 |\n",
    "| `[^A-Za-z]` | Matches any character except letters |\n",
    "| `[]+` | One or more occurrences of characters in brackets |\n",
    "| `[a-zA-Z0-9]+` | One or more alphanumeric characters |\n",
    "| `[a-fA-F0-9]+` | One or more hexadecimal digits |\n",
    "| \\W+ => [^a-zA-Z0-9_] | This is a special regex character class that matches any non-word characters|\n",
    "\n",
    "| String Methods | Description |\n",
    "|----------------|-------------|\n",
    "| `input_string.lower()` | Converts string to lowercase |\n",
    "| `str.lower()` | Converts string to lowercase |\n",
    "| `input_string.strip()` | Removes leading/trailing whitespace |\n",
    "| `str.strip()` | Removes leading/trailing whitespace |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a554a72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found\n",
      "himself transformed in his bed into a horrible vermin. He lay on his\n",
      "armour-like back, and if he lifted his head a little he could se\n"
     ]
    }
   ],
   "source": [
    "# load text\n",
    "filename = 'data/metamorphosis_clean.txt'\n",
    "with open(filename, 'r', encoding='utf-8-sig') as file:\n",
    "    text = file.read()\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea1009a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning,', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'He', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'His', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', '“What’s', 'happened', 'to', 'me?”', 'he', 'thought.', 'It', 'wasn’t', 'a', 'dream.', 'His', 'room,', 'a', 'proper', 'human']\n"
     ]
    }
   ],
   "source": [
    "# split into words by white space\n",
    "words = text.split()\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2878a482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasn', 't', 'a', 'dream', 'His', 'room']\n"
     ]
    }
   ],
   "source": [
    "# using re package to split\n",
    "import re\n",
    "\n",
    "words = re.split(r'\\W+', text)\n",
    "print(words[:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
